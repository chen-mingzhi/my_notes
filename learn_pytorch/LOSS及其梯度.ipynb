{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62ad92d6",
   "metadata": {},
   "source": [
    "均方差：Mean Squared Error(简写为：MSE)\n",
    "\n",
    "$ \\sum \\left (  y - \\bar{y}\\right )^2$\n",
    "\n",
    "其中，$\\bar{y}$ 是模型的预测值"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4881cd9c",
   "metadata": {},
   "source": [
    "线性均方差：$loss = \\sum \\left [ y - \\left ( xw + b \\right )  \\right ] ^2$\n",
    "\n",
    "它长得有些像L2范数但是它没有开根号\n",
    "\n",
    "L2-norm = $\\left \\| y - \\left( xw+b \\right) \\right \\| _{_{2} } $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc47608",
   "metadata": {},
   "source": [
    "所以，线性均方差也可以等于它的L2范数的平方\n",
    "\n",
    "$loss = norm \\left( y - \\left( xw + b \\right ) \\right)^2$\n",
    "\n",
    "torch.norm(y - $Pred$, 2).pow(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7368c879",
   "metadata": {},
   "source": [
    "# MSE的求导\n",
    "\n",
    "![](./img/mse.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911cb794",
   "metadata": {},
   "source": [
    "## pytorch的自动求导(autograd.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c92981b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Pred = x w + b\n",
    "x = torch.ones(1)  # 其中x初始化为1，w\n",
    "w = torch.full([1], 2.)  # w初始化为dim = 1，长度为1的tensor\n",
    "# 没有使用b，这里b = 0\n",
    "mse = F.mse_loss(torch.ones(1), x * w)  # 第一个参数为Pred, 第二个为label，因为是平方所以反过来写也没问题，但建议按顺序写\n",
    "# （1 - 2）^2 = 1\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b2059c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mw\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\My_data\\Work\\Anaconda\\envs\\torch-gpu\\lib\\site-packages\\torch\\autograd\\__init__.py:303\u001b[0m, in \u001b[0;36mgrad\u001b[1;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(grad_outputs_)\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    304\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "torch.autograd.grad(mse, [w])  # 第一个参数是Pred(y), 第二个参数是[w1, w2, w3, ……]（[x1, x2, x3, ……]）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edef46c",
   "metadata": {},
   "source": [
    "报错意思是你的w是不需要求导的但你求了导\n",
    "\n",
    "出现这个错误的根本原因是初始化w的时候没有设置为需要求导信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff8d2629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.], requires_grad=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.requires_grad_() # 对w进行更新，说明w需要求导"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8987db63",
   "metadata": {},
   "source": [
    "### requires_grad_与requires_grad的区别（摘自ChatGPT）\n",
    "\n",
    "在PyTorch中，`requires_grad`是一个布尔值，它指示一个张量是否需要计算梯度。当`requires_grad=True`时，PyTorch会跟踪该张量的计算历史并计算梯度，以便进行反向传播以进行自动微分。默认情况下，`requires_grad`为`False`。\n",
    "\n",
    "`requires_grad_`是一个in-place方法，它可以用来更改一个张量的`requires_grad`属性。与`requires_grad`不同，`requires_grad_`允许您直接修改张量的属性，而不是创建一个新的张量。例如，如果您有一个张量`x`，您可以使用`x.requires_grad_(True)`来将`x`的`requires_grad`属性设置为`True`。\n",
    "\n",
    "需要注意的是，`requires_grad_`方法返回的是原始张量本身，并且如果对原始张量进行in-place操作，则梯度也会相应地更改。因此，在使用`requires_grad_`方法时需要小心，以确保梯度计算的正确性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05ea16a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mw\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\My_data\\Work\\Anaconda\\envs\\torch-gpu\\lib\\site-packages\\torch\\autograd\\__init__.py:303\u001b[0m, in \u001b[0;36mgrad\u001b[1;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(grad_outputs_)\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    304\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "torch.autograd.grad(mse, [w])\n",
    "# 出错是因为mse中的w没更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1c0a636",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2.]),)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse = F.mse_loss(torch.ones(1), x * w)\n",
    "torch.autograd.grad(mse, [w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85a2c56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
